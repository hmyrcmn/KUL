{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmyrcmn/KUL/blob/main/yeniDENEME2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XklicHU61CVB",
        "outputId": "436d3b2b-b532-4858-a766-88c2edd8b7f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.62.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.25.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "zp-AbDMx0ut2",
        "outputId": "914d9666-7177-464a-b5d5-1196aac5a56d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KUL'...\n",
            "remote: Enumerating objects: 94575, done.\u001b[K\n",
            "remote: Counting objects: 100% (24007/24007), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2222/2222), done.\u001b[K\n",
            "remote: Total 94575 (delta 21786), reused 23996 (delta 21781), pack-reused 70568\u001b[K\n",
            "Receiving objects: 100% (94575/94575), 109.16 MiB | 16.78 MiB/s, done.\n",
            "Resolving deltas: 100% (92096/92096), done.\n",
            "Updating files: 100% (28150/28150), done.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'shuffle' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-43d70d4953b0>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# X_data ve Y_labels dizilerini karıştırın\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'shuffle' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "!git clone https://github.com/hmyrcmn/KUL.git\n",
        "\n",
        "# Veri yükleme\n",
        "true_folder ='/content/KUL/BigData/TrueValues'\n",
        "X_data = []\n",
        "\n",
        "Y_labels = []\n",
        "\n",
        "for filename in os.listdir(true_folder):\n",
        "    data = np.genfromtxt(os.path.join(true_folder, filename), delimiter=',')\n",
        "    # X_data.append(data)\n",
        "    X_data.append(data[:, 1:])  # Sadece virgül sonrasındaki değerleri ekledim\n",
        "\n",
        "    Y_labels.append(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "false_folder = '/content/KUL/BigData/FalseValues'\n",
        "for filename in os.listdir(false_folder):\n",
        "    data = np.genfromtxt(os.path.join(false_folder, filename), delimiter=',')\n",
        "    # X_data.append(data)\n",
        "    X_data.append(data[:, 1:])  # Sadece virgül sonrasındaki değerleri ekledim\n",
        "\n",
        "    Y_labels.append(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_data = np.array(X_data)\n",
        "Y_labels = np.array(Y_labels)\n",
        "\n",
        "# X_data ve Y_labels dizilerini karıştırın\n",
        "X_data, Y_labels = shuffle(X_data, Y_labels)\n",
        "\n",
        "\n",
        "print(\"x_data shape: \",X_data.shape)\n",
        "# X_data=X_data,x2_data\n",
        "X_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verileri düzenleme ve normalleştirme\n",
        "num_samples = len(X_data)\n",
        "sequence_length = X_data.shape[1]\n",
        "num_features = X_data.shape[2]\n",
        "\n",
        "X_data_normalized = X_data.reshape(num_samples, sequence_length * num_features)\n",
        "\n",
        "# Eğitim ve test setlerine bölme\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_normalized, Y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train.shape:\",X_train.shape)\n",
        "X_train"
      ],
      "metadata": {
        "id": "de6_iL8O1GH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# TensorBoard için bir log dizini oluşturun\n",
        "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "print(f\"TensorBoard Link: https://tensorboard.dev/experiment/{log_dir}\")\n",
        "\n",
        "# TensorBoard Callback'ini oluşturun\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Model oluşturma\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1000, 1)))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(100, activation='sigmoid'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Model derleme\n",
        "model.compile(optimizer='Nadam', loss='mean_squared_error', metrics=['mse','accuracy'])\n",
        "\n",
        "# Model özetini yazdırma\n",
        "model.summary()\n",
        "\n",
        "# Modeli eğitme ve TensorBoard geri çağırımını kullanma\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=128, validation_split=0.2, callbacks=[tensorboard_callback])\n",
        "\n",
        "# Diğer değerlendirmeleri yapma ve sonuçları yazdırma\n",
        "test_loss, test_mse, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test MSE: {test_mse}, Test Accuracy: {test_acc}')\n",
        "\n",
        "# Model özetini yazdırma\n",
        "model.summary()\n",
        "\n",
        "# Test seti üzerinde tahminler yapma\n",
        "y_pred_probabilities = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred_probabilities, axis=1)\n",
        "\n",
        "# Tahmin edilen sınıfları ve olasılıkları yazdırma\n",
        "for i in range(5):\n",
        "    print(f\"Actual Class: {y_test[i]}, Predicted Class: {y_pred_classes[i]}, Probabilities: {y_pred_probabilities[i]}\")\n",
        "\n",
        "\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "model.fit(X_train, y_train, epochs=60, batch_size=128, validation_split=0.2, callbacks=[tensorboard_callback])\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OO1DQMk81GEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Modelin tahminlerini alın\n",
        "y_pred_proba = model.predict(X_test)\n",
        "y_pred = (y_pred_proba[:, 1] > 0.5).astype(int)  # Eşik değeri üzerinden sınıflara dönüştürme\n",
        "\n",
        "# Confusion matrix oluşturun\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Confusion matrix'i seaborn ile görselleştirme\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds', xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2uEyOBlz1GBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# !git clone https://github.com/hmyrcmn/KUL.git\n",
        "\n",
        "# Veri yükleme\n",
        "true_folder = '/content/KUL/BigData/TrueValues'\n",
        "X_data1 = []\n",
        "X_data2 = []\n",
        "\n",
        "Y_labels = []\n",
        "Y_labels2 = []\n",
        "\n",
        "\n",
        "for filename in os.listdir(true_folder):\n",
        "    data = np.genfromtxt(os.path.join(true_folder, filename), delimiter=',')\n",
        "    # X_data.append(data)\n",
        "    X_data1.append(data[:, 1:])  # Sadece virgül sonrasındaki değerleri ekledim\n",
        "\n",
        "    Y_labels.append(1)\n",
        "\n",
        "false_folder = '/content/KUL/BigData/FalseValues'\n",
        "for filename in os.listdir(false_folder):\n",
        "    data = np.genfromtxt(os.path.join(false_folder, filename), delimiter=',')\n",
        "    # X_data.append(data)\n",
        "    X_data2.append(data[:, 1:])  # Sadece virgül sonrasındaki değerleri ekledim\n",
        "\n",
        "    Y_labels2.append(0)\n",
        "\n",
        "X_data1 = np.array(X_data1)\n",
        "X_data2 = np.array(X_data2)\n",
        "\n",
        "Y_labels = np.array(Y_labels)\n",
        "Y_labels2 = np.array(Y_labels2)\n"
      ],
      "metadata": {
        "id": "efCTl3w91rjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DOpkYHOV1reF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}