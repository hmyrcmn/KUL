{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP+XTN8aGe0en24GMMB+Ro4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmyrcmn/KUL/blob/main/bestSuccest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LyUVv4hZrht"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "!git clone https://github.com/hmyrcmn/KUL.git\n",
        "\n",
        "# Veri yükleme\n",
        "true_folder = '/content/KUL/data/trueValues'\n",
        "X_data = []\n",
        "Y_labels = []\n",
        "\n",
        "for filename in os.listdir(true_folder):\n",
        "    data = np.genfromtxt(os.path.join(true_folder, filename), delimiter=',')\n",
        "    # X_data.append(data)\n",
        "    X_data.append(data[:, 1:])  # Sadece virgül sonrasındaki değerleri ekledim\n",
        "\n",
        "    Y_labels.append(1)\n",
        "\n",
        "\n",
        "\n",
        "true_folder2 = '/content/KUL/truedata'\n",
        "\n",
        "for filename in os.listdir(true_folder2):\n",
        "    data = np.genfromtxt(os.path.join(true_folder2, filename), delimiter=',')\n",
        "    # X_data.append(data)\n",
        "    X_data.append(data[:, 1:])  # Sadece virgül sonrasındaki değerleri ekledim\n",
        "\n",
        "    Y_labels.append(1)\n",
        "\n",
        "true_folder3= '/content/KUL/data/trueValues2'\n",
        "\n",
        "for filename in os.listdir(true_folder3):\n",
        "    data = np.genfromtxt(os.path.join(true_folder3, filename), delimiter=',')\n",
        "    # X_data.append(data)\n",
        "    X_data.append(data[:, 1:])  # Sadece virgül sonrasındaki değerleri ekledim\n",
        "\n",
        "    Y_labels.append(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "false_folder = '/content/KUL/data/falseValues'\n",
        "for filename in os.listdir(false_folder):\n",
        "    data = np.genfromtxt(os.path.join(false_folder, filename), delimiter=',')\n",
        "    # X_data.append(data)\n",
        "    X_data.append(data[:, 1:])  # Sadece virgül sonrasındaki değerleri ekledim\n",
        "\n",
        "    Y_labels.append(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "false_folder2 = '/content/KUL/falsedata'\n",
        "\n",
        "for filename in os.listdir(false_folder2):\n",
        "    data = np.genfromtxt(os.path.join(false_folder2, filename), delimiter=',')\n",
        "    # X_data.append(data)\n",
        "    X_data.append(data[:, 1:])  # Sadece virgül sonrasındaki değerleri ekledim\n",
        "\n",
        "    Y_labels.append(0)\n",
        "\n",
        "false_folder3 = '/content/KUL/data/falseValues2'\n",
        "\n",
        "for filename in os.listdir(false_folder3):\n",
        "    data = np.genfromtxt(os.path.join(false_folder3, filename), delimiter=',')\n",
        "    # X_data.append(data)\n",
        "    X_data.append(data[:, 1:])  # Sadece virgül sonrasındaki değerleri ekledim\n",
        "\n",
        "    Y_labels.append(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_data = np.array(X_data)\n",
        "Y_labels = np.array(Y_labels)\n",
        "print(\"x_data shape: \",X_data.shape)\n",
        "X_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StandardScaler"
      ],
      "metadata": {
        "id": "1z36fk6vgsMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Verileri düzleştirme ve normalize etme\n",
        "X_data_flattened = X_data.reshape(X_data.shape[0], -1)\n",
        "scaler = StandardScaler()\n",
        "X_data_standardized = scaler.fit_transform(X_data_flattened)\n",
        "\n",
        "# Verilerin ilk örneğini gösterme\n",
        "print(\"Önceki Veri:\")\n",
        "print(X_data[0])\n",
        "print(\"\\nSonraki Veri (Standart Normalize Edilmiş):\")\n",
        "print(X_data_standardized[0])\n",
        "\n",
        "# Standart normalize edilmiş verinin histogramını çizme\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(X_data_standardized.flatten(), bins=50, color='blue', alpha=0.7)\n",
        "plt.title('Standardized Data Histogram')\n",
        "plt.xlabel('Standardized Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_standardized, Y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n"
      ],
      "metadata": {
        "id": "Qd5LlXINaRwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34mq-kW6dNN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROBUST SCALER"
      ],
      "metadata": {
        "id": "PXRVQznHd7M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Verileri düzleştirme ve normalize etme\n",
        "X_data_flattened = X_data.reshape(X_data.shape[0], -1)\n",
        "\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_data_standardized = scaler.fit_transform(X_data_flattened)\n",
        "\n",
        "\n",
        "\n",
        "# Verilerin ilk örneğini gösterme\n",
        "print(\"Önceki Veri:\")\n",
        "print(X_data[0])\n",
        "print(\"\\nSonraki Veri (Standart Normalize Edilmiş):\")\n",
        "print(X_data_standardized[0])\n",
        "\n",
        "# Standart normalize edilmiş verinin histogramını çizme\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(X_data_standardized.flatten(), bins=50, color='red', alpha=0.7)\n",
        "plt.title('Standardized Data Histogram')\n",
        "plt.xlabel('Standardized Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_standardized, Y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n"
      ],
      "metadata": {
        "id": "mxzbYAFwdNER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# min max sclaer"
      ],
      "metadata": {
        "id": "u9Z3CyioLxUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Verileri düzleştirme ve normalize etme\n",
        "X_data_flattened = X_data.reshape(X_data.shape[0], -1)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Tüm veriyi normalize edin\n",
        "veri_normal = scaler.fit_transform(X_data_flattened)\n",
        "\n",
        "\n",
        "# Verilerin ilk örneğini gösterme\n",
        "print(\"Önceki Veri:\")\n",
        "print(X_data[0])\n",
        "print(\"\\nSonraki Veri (Standart Normalize Edilmiş):\")\n",
        "print(X_data_standardized[0])\n",
        "\n",
        "# Standart normalize edilmiş verinin histogramını çizme\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(X_data_standardized.flatten(), bins=50, color='red', alpha=0.7)\n",
        "plt.title('Standardized Data Histogram')\n",
        "plt.xlabel('Standardized Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_standardized, Y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n"
      ],
      "metadata": {
        "id": "bWgnn_T6LwvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PowerTransformer"
      ],
      "metadata": {
        "id": "Jgqm8QIlStVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Verileri düzleştirme ve normalize etme\n",
        "X_data_flattened = X_data.reshape(X_data.shape[0], -1)\n",
        "\n",
        "power_transformer = PowerTransformer(method='yeo-johnson')\n",
        "sinyal_normal = power_transformer.fit_transform(X_data_flattened)\n",
        "\n",
        "\n",
        "# Verilerin ilk örneğini gösterme\n",
        "print(\"Önceki Veri:\")\n",
        "print(X_data[0])\n",
        "print(\"\\nSonraki Veri (Standart Normalize Edilmiş):\")\n",
        "print(X_data_standardized[0])\n",
        "\n",
        "# Standart normalize edilmiş verinin histogramını çizme\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(X_data_standardized.flatten(), bins=50, color='red', alpha=0.7)\n",
        "plt.title('Standardized Data Histogram')\n",
        "plt.xlabel('Standardized Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_standardized, Y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n"
      ],
      "metadata": {
        "id": "mmvyMsxOSuem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QuantileTransformer\n"
      ],
      "metadata": {
        "id": "dpe1kutFSZwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "# Verileri düzleştirme ve normalize etme\n",
        "X_data_flattened = X_data.reshape(X_data.shape[0], -1)\n",
        "\n",
        "# Quantile Transformer kullanarak normalizasyon yapın\n",
        "quantile_transformer = QuantileTransformer(output_distribution='uniform')\n",
        "sinyal_normal = quantile_transformer.fit_transform(X_data_flattened)\n",
        "\n",
        "\n",
        "# Verilerin ilk örneğini gösterme\n",
        "print(\"Önceki Veri:\")\n",
        "print(X_data[0])\n",
        "print(\"\\nSonraki Veri (Standart Normalize Edilmiş):\")\n",
        "print(X_data_standardized[0])\n",
        "\n",
        "# Standart normalize edilmiş verinin histogramını çizme\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(X_data_standardized.flatten(), bins=50, color='red', alpha=0.7)\n",
        "plt.title('Standardized Data Histogram')\n",
        "plt.xlabel('Standardized Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_standardized, Y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n"
      ],
      "metadata": {
        "id": "x3dN9OWhSatK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3VqrM2HDSarB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# z-score normalzation"
      ],
      "metadata": {
        "id": "qBVMZ92aK1ft"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CMzt7ONNd1hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Verileri düzleştirme ve normalize etme\n",
        "X_data_flattened = X_data.reshape(X_data.shape[0], -1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Tüm veriyi normalize edin\n",
        "veri_normal = scaler.fit_transform(X_data_flattened)\n",
        "\n",
        "\n",
        "# Verilerin ilk örneğini gösterme\n",
        "print(\"Önceki Veri:\")\n",
        "print(X_data[0])\n",
        "print(\"\\nSonraki Veri (Standart Normalize Edilmiş):\")\n",
        "print(X_data_standardized[0])\n",
        "\n",
        "# Standart normalize edilmiş verinin histogramını çizme\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(X_data_standardized.flatten(), bins=50, color='red', alpha=0.7)\n",
        "plt.title('Standardized Data Histogram')\n",
        "plt.xlabel('Standardized Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_standardized, Y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n"
      ],
      "metadata": {
        "id": "Wt4E5kl_K7S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train model\n"
      ],
      "metadata": {
        "id": "HCtYMGNCMVH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(32, kernel_size=5, activation='relu', input_shape=(100, 1)))\n",
        "model.add(Flatten())\n",
        "\n",
        "\n",
        "model.add(Dense(64, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "# Dropout ekleyebilirsiniz\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "# model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "# model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
        "# model.compile(optimizer=\"adam\", loss=\"hinge\", metrics=['accuracy'])\n",
        "# model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=['mse'])\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Modeli eğitme\n",
        "model.fit(X_train, y_train, epochs=60, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Modelin performansını değerlendirme\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "# Model özetini yazdırma\n",
        "model.summary()\n",
        "\n",
        "# Test seti üzerinde tahminler yap\n",
        "y_pred_probabilities = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred_probabilities, axis=1)\n",
        "\n",
        "# Tahmin edilen sınıfları ve olasılıkları yazdır\n",
        "say = 0\n",
        "for i in range(len(y_test)):\n",
        "    if say < 5:\n",
        "        say += 1\n",
        "        print(f\"Gerçek Sınıf: {y_test[i]}, Tahmin Edilen Sınıf: {y_pred_classes[i]}, Olasılıklar: {y_pred_probabilities[i]}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rsX0ji0PaRuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# show confusion matrix"
      ],
      "metadata": {
        "id": "H6kcPMQZMa9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Modelin tahminlerini alın\n",
        "y_pred_proba = model.predict(X_test)\n",
        "y_pred = (y_pred_proba[:, 1] > 0.5).astype(int)  # Eşik değeri üzerinden sınıflara dönüştürme\n",
        "\n",
        "# Confusion matrix oluşturun\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Confusion matrix'i seaborn ile görselleştirme\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds', xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AHRK-PwZaRrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sp2JbNiVMgcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test the model"
      ],
      "metadata": {
        "id": "XDlZdaO2MgxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# Veri yükleme\n",
        "true_folder = '/content/KUL/data/testmodel/true'\n",
        "X_data2 = []\n",
        "Y_labels2 = []\n",
        "\n",
        "for filename in os.listdir(true_folder):\n",
        "    data2 = np.genfromtxt(os.path.join(true_folder, filename), delimiter=',')\n",
        "    X_data2.append(data2[:, 1:])\n",
        "    Y_labels2.append(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "true_folder2 = '/content/KUL/data/testmodel/true2'\n",
        "\n",
        "\n",
        "for filename in os.listdir(true_folder2):\n",
        "    data2 = np.genfromtxt(os.path.join(true_folder2, filename), delimiter=',')\n",
        "    X_data2.append(data2[:, 1:])\n",
        "    Y_labels2.append(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "false_folder = '/content/KUL/data/testmodel/false'\n",
        "for filename in os.listdir(false_folder):\n",
        "    data2 = np.genfromtxt(os.path.join(false_folder, filename), delimiter=',')\n",
        "    X_data2.append(data2[:, 1:])\n",
        "    Y_labels2.append(0)\n",
        "\n",
        "\n",
        "\n",
        "false_folder2 = '/content/KUL/data/testmodel/false2'\n",
        "for filename in os.listdir(false_folder2):\n",
        "    data2 = np.genfromtxt(os.path.join(false_folder2, filename), delimiter=',')\n",
        "    X_data2.append(data2[:, 1:])\n",
        "    Y_labels2.append(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_data2 = np.array(X_data2)\n",
        "Y_labels2 = np.array(Y_labels2)\n",
        "print(\"X_data shape:\", X_data2.shape)\n",
        "\n",
        "# Verileri düzenleme ve normalize etme\n",
        "num_samples = len(X_data2)\n",
        "sequence_length = X_data2.shape[1]\n",
        "num_features = X_data2.shape[2]\n",
        "\n",
        "# X_data2'yi düzenleme\n",
        "X_data2_reshaped = X_data2.reshape(num_samples, sequence_length * num_features)\n",
        "\n",
        "# StandardScaler kullanarak normalleştirme\n",
        "scaler = StandardScaler()\n",
        "X_data2_standardized = scaler.fit_transform(X_data2_reshaped)\n",
        "\n",
        "# Normalleştirilmiş verilerin boyutunu kontrol etme\n",
        "print(\"X_data2_standardized shape:\", X_data2_standardized.shape)\n",
        "\n",
        "# Y_labels2'yi kontrol etme\n",
        "print(\"Y_labels2 shape:\", Y_labels2.shape)\n",
        "\n",
        "# Model ile tahmin yapma\n",
        "predictions = model.predict(X_data2_standardized)\n",
        "# Tahmin sonuçlarını eşik değere göre ikili sınıflara dönüştürme\n",
        "binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "# # Her bir örnek için en yüksek tahmin oranına sahip sınıfı ve olasılığı yazdırma\n",
        "# for i in range(len(binary_predictions)):\n",
        "#     predicted_class = np.argmax(predictions[1])  # En yüksek tahmin oranına sahip sınıfı al\n",
        "#     probability = predictions[i][predicted_class]  # Tahmin edilen sınıfa ait olasılığı al\n",
        "#     print(f\"Sample {i + 1}: Gerçek Değer: {Y_labels2[i]}, Predicted Class: {predicted_class}, Probability: {probability:.4f}\")\n",
        "\n",
        "# # binary_predictions\n",
        "y_new_pred_probabilities = model.predict(X_data2_standardized)\n",
        "y_new_pred_classes = np.argmax(y_new_pred_probabilities, axis=1)\n",
        "\n",
        "# Tahmin edilen sınıfları ve olasılıkları yazdırma\n",
        "for i in range(len(y_new_pred_classes)):\n",
        "    print(f\"Dosya {i + 1}:Gerçek Değer: {Y_labels2[i]}, Tahmin Edilen Sınıf: {y_new_pred_classes[i]}, Olasılıklar: {y_new_pred_probabilities[i]}\")"
      ],
      "metadata": {
        "id": "LwY7dsRNaS_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tahmin edilen sınıfları ve olasılıkları yazdırma\n",
        "for i in range(len(y_new_pred_classes)):\n",
        "    predicted_class = np.argmax(y_new_pred_probabilities[i])\n",
        "    probability = y_new_pred_probabilities[i][predicted_class]\n",
        "    print(f\"Dosya {i + 1}:Gerçek Değer: {Y_labels2[i]},   Tahmin Edilen Sınıf: {predicted_class}, Olasılıklar: {probability}\")\n"
      ],
      "metadata": {
        "id": "4l6XpXukaS8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXC3_ZBO9jF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VEW-WV6h9jDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# show data"
      ],
      "metadata": {
        "id": "z_4tri35MmRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the paths to your text files\n",
        "true = '/content/KUL/truedata/smooth2001_0.txt'\n",
        "false = '/content/KUL/falsedata/smooth2003_0.txt'\n",
        "true1 = '/content/KUL/truedata2/smooth2004_0.txt'\n",
        "false1 = '/content/KUL/falsedata2/smooth2005_0.txt'\n",
        "bes=\"/content/KUL/data/falseValues/smooth2001_0.txt\"\n",
        "altı=\"/content/KUL/data/falseValues2/smooth20011_0.txt\"\n",
        "yedi=\"/content/KUL/data/trueValues/smooth2001_0.txt\"\n",
        "sekız=\"/content/KUL/data/trueValues2/smooth20010_10.txt\"\n",
        "# Load data from the first text file\n",
        "data1 = np.genfromtxt(true, delimiter=',')\n",
        "y_values1 = data1[:, 1]\n",
        "\n",
        "# Load data from the second text file\n",
        "data2 = np.genfromtxt(false, delimiter=',')\n",
        "y_values2 = data2[:, 1]\n",
        "\n",
        "\n",
        "# Load data from the first text file\n",
        "data3 = np.genfromtxt(true1, delimiter=',')\n",
        "y_values3 = data3[:, 1]\n",
        "\n",
        "# Load data from the second text file\n",
        "data4 = np.genfromtxt(false1, delimiter=',')\n",
        "y_values4 = data4[:, 1]\n",
        "\n",
        "\n",
        "\n",
        "# Load data from the first text file\n",
        "data5= np.genfromtxt(bes, delimiter=',')\n",
        "y_values5 = data5[:, 1]\n",
        "\n",
        "# Load data from the second text file\n",
        "data6 = np.genfromtxt(altı, delimiter=',')\n",
        "y_values6 = data6[:, 1]\n",
        "# Load data from the first text file\n",
        "data7 = np.genfromtxt(yedi, delimiter=',')\n",
        "y_values7 = data7[:, 1]\n",
        "\n",
        "# Load data from the second text file\n",
        "data8 = np.genfromtxt(sekız, delimiter=',')\n",
        "y_values8 = data8[:, 1]\n",
        "\n",
        "# Plot the data from both files on the same graph\n",
        "plt.plot(y_values1, label='true')\n",
        "plt.plot(y_values2, label='false')\n",
        "plt.plot(y_values3, label='true1')\n",
        "plt.plot(y_values4, label='false1')\n",
        "\n",
        "\n",
        "plt.plot(y_values5, label='bes')\n",
        "plt.plot(y_values6, label='altı')\n",
        "plt.plot(y_values7, label='yedi')\n",
        "plt.plot(y_values8, label='sekız')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Data Point Index')\n",
        "plt.ylabel('Y Values')\n",
        "plt.title('Data from Two Text Files')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KpWGhokL9jA2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}